{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9lsfksnKuwI2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install unsloth\n",
        "!pip install vllm\n",
        "!pip install cairosvg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKmzAPLcTwoT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/dev-files')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u57-uL0gu2d6"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import choice, randint\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Utilities for plotting\n",
        "from symbolic_utilities import progress, compute_global_limits_smc, plot_mh_trace_upto, plot_state_2d\n",
        "# MHMC sampler\n",
        "from symbolic_utilities import propose_tree, get_coordinates, \\\n",
        "    mh_sampler, smc_sampler, define_bs_DSL, define_lt_DSL, enumerate_full_sentences\n",
        "\n",
        "from neural_utilities import extract_xml_answer, extract_xml_reasoning, produce_tasks, get_data\n",
        "\n",
        "from neural_utilities import print_func, lt_correctness_reward_func, \\\n",
        "    xmlcount_reward_func, soft_format_reward_func, strict_format_reward_func, cfg_reward_func, lt_correctness_reward_func, \\\n",
        "    direct_cfg_reward_func, direct_lt_correctness_reward_func, direct_conciseness_reward_func\n",
        "\n",
        "from symbolic_utilities import \\\n",
        "    ltgrammar, lt_nonterminals, lt_terminals, lt_eval_dict, \\\n",
        "    bsgrammar, bs_nonterminals, bs_terminals, bs_eval_dict\n",
        "\n",
        "from evaluation import generate_answers, select_best, get_accuracy\n",
        "\n",
        "# NOTE: PatchFastRL needs to run **before** the imports below\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported, PatchFastRL\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
        "\n",
        "import torch, gc\n",
        "from torch import tensor\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import EarlyStoppingCallback, TextStreamer, TrainingArguments\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from trl import SFTTrainer, GRPOConfig, GRPOTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from vllm import SamplingParams\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ1N-hptvLMQ"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 64 # Larger rank = smarter, but slower\n",
        "\n",
        "lt_system_prompt = \"\"\n",
        "\n",
        "# get all sentences up to depth 5\n",
        "sentences_pool = []\n",
        "for i, sent in enumerate(enumerate_full_sentences('T', ltgrammar, max_depth=6)):\n",
        "    sentences_pool.append(sent)\n",
        "    if i==500000:\n",
        "        break\n",
        "\n",
        "print(sentences_pool[:5])\n",
        "\n",
        "data = get_data(\n",
        "    ltgrammar,\n",
        "    lt_system_prompt,\n",
        "    eval_dict=lt_eval_dict,\n",
        "    n_tasks=100000,\n",
        "    sentences_pool=sentences_pool\n",
        ")\n",
        "\n",
        "# the 'sentence' is what we want the model to output for a given input.\n",
        "data = data.map(lambda x: {\n",
        "    'completion': [{'content': x['sentence'], 'role': 'assistant'}],\n",
        "})\n",
        "\n",
        "# 90% train, 10% test + validation\n",
        "train_testvalid = data.train_test_split(train_size=2**16, test_size=2* 2**7)\n",
        "# Split the 10% test + valid in half test, half valid\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "# gather everyone if you want to have a single DatasetDict\n",
        "data = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw3eYHK11kI2"
      },
      "source": [
        "sentence: target program\\\n",
        "e.g. filter_(and_(or_(gt(2),gt(2)),or_(even,even)))\n",
        "\n",
        "examples:\n",
        "[[[5, 5, 8], [8]], [[3, 4], [4]], ...] - shows input and output pairs\n",
        "\n",
        "task:\n",
        "human-readable version of the examples which could be directly fed into the prompt\n",
        "\n",
        "prompt:\n",
        "the prompt containing `task` in a LLM friendly manner\n",
        "\n",
        "completions: the sentence according to the task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AOdRrevy9tu"
      },
      "outputs": [],
      "source": [
        "d = data['train'][0]['prompt']\n",
        "# data['train'][0]['completion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnLpiwy70wTB"
      },
      "outputs": [],
      "source": [
        "c = Counter()\n",
        "for s in data['train']['sentence']:\n",
        "   c.update({k:s.count(k) for k in lt_terminals})\n",
        "\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEtY9u9E3V1j"
      },
      "outputs": [],
      "source": [
        "data['test'].to_pandas().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN6F9IGLNgS0"
      },
      "source": [
        "## start building model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEmxHvr-3mJE"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    # fast_inference=True,\n",
        "    max_lora_rank=lora_rank,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state=3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjJC-HK94OE3"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    d,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Rggtkj5wKD"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(\n",
        "    inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr02NI0_6hhE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "FastLanguageModel.for_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpJ-atck-Pey"
      },
      "outputs": [],
      "source": [
        "# Format function\n",
        "def format_dataset(examples):\n",
        "    texts = []\n",
        "    for prompt_msgs, completion_msgs in zip(examples[\"prompt\"], examples[\"completion\"]):\n",
        "        # Combine prompt and completion messages\n",
        "        messages = prompt_msgs + completion_msgs\n",
        "\n",
        "        # Filter out empty system messages\n",
        "        messages = [msg for msg in messages if msg[\"content\"].strip()]\n",
        "\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting to create the \"text\" column\n",
        "formatted_train = data['train'].map(format_dataset, batched=True, remove_columns=data['train'].column_names)\n",
        "formatted_test = data['test'].map(format_dataset, batched=True, remove_columns=data['test'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgZYmEP19h0Y"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train,\n",
        "    eval_dataset=formatted_test,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=SFTConfig(\n",
        "        learning_rate=3e-5,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=0.4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.05,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"lt_SFT_noreasoning\",\n",
        "        seed=0,\n",
        "        save_steps=100,\n",
        "        fp16_full_eval=True,\n",
        "        per_device_eval_batch_size=2,\n",
        "        eval_accumulation_steps=4,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=10,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WixFVze_BSzR"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz6ni_JdG6__"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('finetuned_lt')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !cp -r /content/finetuned_lt/ /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrDHO7wSH9TA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"/content/drive/MyDrive/dev-files/finetuned_lt\", # \"finetuned_lt\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    # Enable vLLM fast inference\n",
        "    # fast_inference = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZqxcpxpH90L"
      },
      "outputs": [],
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "\n",
        "prompts = [\n",
        "    tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False)\n",
        "    for p in data['test']['prompt']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRitdqttKgXf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1hGTMW4iGPj"
      },
      "outputs": [],
      "source": [
        "answers = generate_answers(model, tokenizer, data['test']['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpDYftCAY2Oe"
      },
      "outputs": [],
      "source": [
        "get_accuracy(answers, data['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De8q4c9BdzZE"
      },
      "source": [
        "## now let's start using RL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrD3SWeuZcFV"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        # print_func,\n",
        "        # direct_cfg_reward_func,\n",
        "        direct_lt_correctness_reward_func,\n",
        "        # direct_conciseness_reward_func\n",
        "    ],\n",
        "    args=GRPOConfig(\n",
        "        # use vLLM for fast inference! (it raises an error)\n",
        "        # use_vllm = True,\n",
        "        learning_rate = 5e-6,\n",
        "        adam_beta1 = 0.9,\n",
        "        adam_beta2 = 0.99,\n",
        "        weight_decay = 0.1,\n",
        "        warmup_ratio = 0.1,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        logging_steps = 1,\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        per_device_train_batch_size = 1,\n",
        "        # Increase to 4 for smoother training\n",
        "        gradient_accumulation_steps = 1,\n",
        "        # Decrease if out of memory\n",
        "        num_generations = 8,\n",
        "        max_prompt_length = 256,\n",
        "        max_completion_length = 64,\n",
        "        # Set to 1 for a full training run\n",
        "        num_train_epochs = 1,\n",
        "        max_steps = 10000,\n",
        "        save_steps = 500,\n",
        "        output_dir=\"/content/drive/MyDrive/dev-files/grpo_checkpoints\",\n",
        "        max_grad_norm = 0.1,\n",
        "    ),\n",
        "    train_dataset=data['train'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tfZX5zixd8sC"
      },
      "outputs": [],
      "source": [
        "trainer.train(\n",
        "    resume_from_checkpoint=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V7bHSgzpRIzk"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('rl_finetuned')\n",
        "\n",
        "!cp -r /content/rl_finetuned/ /content/drive/MyDrive/dev-files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgc5fCwfhPq8"
      },
      "outputs": [],
      "source": [
        "df_history = pd.DataFrame(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "168d-jgchQmD"
      },
      "outputs": [],
      "source": [
        "smoothed_rewards = df_history['rewards/direct_lt_correctness_reward_func'].rolling(window=100).mean()\n",
        "\n",
        "# Plotting the raw reward and the trend (smoothed reward)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(df_history.index, smoothed_rewards, label=\"Trend (Moving Average)\", color=\"red\", linewidth=2)\n",
        "plt.scatter(df_history.index, df_history['rewards/direct_lt_correctness_reward_func'], s=1)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.ylim(0,3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW2nCKMfhTqZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "model.for_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwQsmOy6iYW-"
      },
      "outputs": [],
      "source": [
        "answers = generate_answers(model, tokenizer, data['test']['prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Ex8KMOicKC"
      },
      "outputs": [],
      "source": [
        "get_accuracy(answers, data['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kLj1S2CNp31"
      },
      "outputs": [],
      "source": [
        "answers = generate_answers(\n",
        "    model, tokenizer,\n",
        "    data['test']['prompt'],\n",
        "    examples=data['test']['examples'],  # pass the I/O examples\n",
        "    eval_dict=lt_eval_dict,\n",
        "    batch_size=4,\n",
        "    num_return_sequences=32,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing eval performance on different model checkpoints\n",
        "\n",
        "Final straw to see what's happening with the reward signal ðŸ˜§"
      ],
      "metadata": {
        "id": "03kc6vsE_Cvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEkTtPT7SHVJ"
      },
      "outputs": [],
      "source": [
        "# Get all checkpoints\n",
        "checkpoint_dir = \"/content/drive/MyDrive/dev-files/grpo_checkpoints\"\n",
        "checkpoints = sorted(\n",
        "    [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")],\n",
        "    key=lambda x: int(x.split(\"-\")[1])\n",
        ")\n",
        "\n",
        "print(f\"Found {len(checkpoints)} checkpoints: {checkpoints}\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for ckpt in tqdm(checkpoints):\n",
        "    ckpt_path = os.path.join(checkpoint_dir, ckpt)\n",
        "    step = int(ckpt.split(\"-\")[1])\n",
        "\n",
        "    # Load model from checkpoint\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=ckpt_path,\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    tokenizer.padding_side = 'left'\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Generate answers on test set - MAYBE NEED TO INCREASE THIS NUMBER!!\n",
        "    test_subset = data['test'].select(range(64))\n",
        "    answers = generate_answers(model, tokenizer, test_subset['prompt'])\n",
        "\n",
        "    # Compute accuracy (using your existing function)\n",
        "    acc = get_accuracy(answers, test_subset)\n",
        "    results[step] = acc\n",
        "    print(f\"Checkpoint {step}: {acc:.4f}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Plot the learning curve\n",
        "steps, accs = zip(*sorted(results.items()))\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(steps, accs, 'o-', markersize=4)\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"GRPO Checkpoint Evaluation\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(\"/content/drive/MyDrive/dev-files/checkpoint_eval_curve.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/dev-files/checkpoint_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMfpPq2_DnvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}